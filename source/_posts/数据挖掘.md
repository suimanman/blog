---
title: 数据挖掘
date: 2024-04-06 10:00:00
author: 岁漫
img: /images/k1.jpg
top: true
hide: false
cover: true
coverImg: /images/k1.jpg
toc: false
mathjax: false
summary: 跟河工智叟学数据挖掘
categories: 大三课程整理
tags:
  - 数据挖掘
---

作业：第三章：3，5，6，7，9。第四章：2，5，6，16。第五章：1、2、4。第六章：3，6，14

## 认识数据

* Data Objects and Attribute Types：数据对象和属性类型
* Basic Statistical Descriptions of Data：数据的基本统计描述
* Proximity Measure for Nominal Attributes：标称属性的邻近性度量
* Data Visualization：数据可视化
  * Pixel-oriented visualization techniques：面向像素
  * Geometric projection visualization techniques：几何投影
  * Icon-based visualization techniques：基于图标
  * Hierarchical visualization techniques：分层可视化
  * Visualizing complex data and relations：可视化复杂的数据和关系
* Measuring Data Similarity and Dissimilarity：测量数据的相似性和相异性
* term-frequency vector：词频向量；Ordinal：序数属性；median：中位数；
* Variance：方差；Standard deviation：标准差
* Estimated by interpolation：对于分组数据通过插值法估计
* Symmetric（Asymmetric） binary：对称（非对称）的二元属性
* Discrete vs. Continuous ：离散与连续；dispersion characteristics：分散特征
* Boxplot or quantile analysis on sorted intervals：盒图或分位数分析
* Weighted arithmetic mean：加权算术平均值；Trimmed mean：截尾平均值
* Inter-quartile range：四分位数范围；Outlier：异常值
* Histogram：直方图；Quantile plot：分位数图；Scatter plot：散点图
* Data Matrix and Dissimilarity Matrix：数据矩阵和相异性矩阵
* Minkowski Distance：闵可夫斯基距离

极差、四分位数和四分位数极差

* 极差：max-min
* 四分位数分别称为Q1、Q2、Q3
* 四分位数极差：IQR=Q3-Q1

盒图-体现了五数概括：

* 盒的端点一般在四分位数上，使得盒的长度是四分位数极差1QR。
* 中位数用盒内的线标记。
* 盒外的两条线（称做胡须）延伸到最小（Minimum）和最大（Maximum）观测值

当处理数量适中的观测值时，值得个别地绘出可能的离群点。在盒图中这样做：仅当最高和最低观测值超过四分位数不到1.5xIQR时，胡须扩展到它们。否则，胡须在出现在四分位数的1.5×IQR之内的最极端的观测值处终止，剩下的情况个别地绘出。盒图可以用来比较若干个可比较的数据集。

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240617121911262.png" alt="image-20240617121911262" style="zoom: 40%;" />

加权平均，均值（mean）

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622152300903.png" alt="image-20240622152300903" style="zoom: 50%;" />

中位数（median）

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622152445803.png" alt="image-20240622152445803" style="zoom:40%;" />

中列数：最大和最小的平均值

方差

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622153242221.png" alt="image-20240622153242221" style="zoom:50%;" />

相异性和相似性都称为临近性

* 数据矩阵：对象-属性结构，存放n*p(n个对象 * p个属性)
* 相异性矩阵：对象-对象结构，存放两两之间的邻近度

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622160148422.png" alt="image-20240622160148422" style="zoom:50%;" />

**二元属性的相异性**：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622161823758.png" alt="image-20240622161823758" style="zoom:50%;" />

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622161847719.png" alt="image-20240622161847719" style="zoom:50%;" />

一般来说，两属性的重要程度不一样，所有可以忽略重要程度低的属性取值相同的情况：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622162051224.png" alt="image-20240622162051224" style="zoom:40%;" />

非对称的二元相似性：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622162203748.png" alt="image-20240622162203748" style="zoom:50%;" />

欧几里得距离：两点之间直线距离

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622163900776.png" alt="image-20240622163900776" style="zoom:60%;" />

曼哈顿距离：城市街区距离，x轴差值+y轴差值

闵可夫斯基距离：又称
$$
Lp
$$
范数，p就是h，欧几里得距离就是L2范数

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622164214251.png" alt="image-20240622164214251" style="zoom:60%;" />

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622164631391.png" alt="image-20240622164631391" style="zoom:50%;" />

**序数属性的相异性**：将其转化为相应的排位（规范到0～1之间）。假设有三个状态：fair、good、excellent。

此时共有4个值：excellent、fair、good、excellent；三个状态则分别映射为0、0.5、1.0；那4个值的相异性矩阵：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622170912818.png" alt="image-20240622170912818" style="zoom:50%;" />

**混合属性的相异性**：（数据集中含有多类型的属性）则两对象i和j之间的相异性定义为：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622171816944.png" alt="image-20240622171816944" style="zoom:40%;" />

所以，我们需要先把每个属性单独的相异性矩阵算出来，再根据上述公式进行叠加：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622172134827.png" alt="image-20240622172134827" style="zoom:40%;" />

余弦相似性：（计算词频向量）

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622200400903.png" alt="image-20240622200400903" style="zoom:50%;" />

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240622200533968.png" alt="image-20240622200533968" style="zoom:50%;" />

# 数据预处理

Data quality: accuracy, completeness, consistency, timeliness, believability, interpretability

## 数据质量三要素

准确性、完整性、一致性、时效性、可信性和可解释性。

## 数据预处理的的主要任务（Major Tasks in Data Preprocessing）

* 数据清理（data cleaning）：例程通过填写缺失的值，光滑噪声数据，识别或删除离群点
* 数据集成（data integration）：代表同一概念的属性在不同的数据库中可能具有不同的名字，导致不一致性和冗余。
* 数据归约（data reduction）：（为分析选取的数据集是巨大的，这会降低数据挖掘的速度）数据规约得到数据集的简化表示，它小得多，但能够产生同样的（或几乎同样的）分析结果。数据归约策略包括维归约和数值归约。
  * 维归约：使用数据编码方案，以便得到原始数据的简化或“压缩”表示。例子包括数据压缩技术（例如，小波变换和主成分分析），以及属性子集选择（例如，去掉不相关的属性）和属性构造（例如，从原来的属性集导出更有用的小属性集）
  * 数值归约：使用参数模型（例如，回归和对数线性模型）或非参数模型（例如，直方图、聚类、抽样或数据聚集），用较小的表示取代数据
* 数据变换（data transformation）规范化、数据离散化和概念分层

## 数据清理

### 处理缺失值（incomplete\missing data）

1. 忽略元组：当缺少类标号时通常这样做（假定挖掘任务涉及分类）。除非元组有多个属性缺少值，否则该方法不是很有效。当每个属性缺失值的百分比变化很大时，它的性能特别差。采用忽略元组，你不能使用该元组的剩余属性值。
2. 人工填写缺失值：缺失很多值时，行不通。
3. 使用一个全局常量填充缺失值：如unknown,∞等代替。
4. 使用**属性**的中心度量（如均值或中位数）填充缺失值：对于正常的（对称的）数据分布而言，可以使用均值，而倾斜数据分布应该使用中位数
5. 使用与**给定元组属于同一类**的所有样本的属性均值或中位数：，如果将顾客按credit_rish 分类，则用具有相同信用风险的顾客的平均收入替换 incore 中的缺失值。如果给定类的数据分布是倾斜的，则中位数是更好的选择。
6. (**最常用**)使用最有可能的值填充：用回归/使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。

***⚠️注意***：缺失值并不意味数据错误，比如要求提供驾照号码，一些人没有驾照，自然不填。

### 处理噪声数据（noisy）

噪声：是被测量的变量的随机误差或方差。

1. 分箱(binning)：将一些有序的值分到不同的箱中，采用箱均值、箱中位数、箱边界方法来光滑“近邻”的值。

   划分为（等频的）箱：箱1：4,8,15、箱2:21.21,24、箱3:25.28,34

   用箱均值光滑：箱1：9,9,9、箱2:22,22,22、箱3：29,29,29

   用箱边界光滑：箱1：4,4,15、箱2:21.21,24、箱3：25,25,34

2. 回归(regression)：使用一种函数拟合数据来光滑数据。

3. 离群点分析(outlier analysis)：通过如聚类来检测离群点。

*许多数据光滑的方法也用于数据离散化和数据归约*

### 不一致的（inconsistent）

之前用的1、2、3，现在是A、B、C

### 故意的（Intentional ）

### 数据清理作为一个过程(Data Cleaning as a Process)

*元数据：关于数据性质的知识（关于数据的数据）*

偏差检测(discrepancy detection)：利用元数据对数据进行检测，还有一些规则：

1. 唯一性规则：给定属性值都必须不同
2. 连续性～：属性的最低和最高值之间没有缺失的值，还必须唯一
3. 空值规则：说明空白、问号、特殊符号等符号的使用

数据迁移和整合（Data migration and integration）：

还有一些商业工具帮助检测：数据清洗工具、数据审计工具、数据迁移工具。

## 数据集成

*合并来自多个数据源的数据，小心集成有助于减少结果数据集的冗余和不一致。提高后期数据处理的准确性和速度。*

### 实体识别问题（Entity identification problem）

对于不同数据源中同一实体的类名、属性名或属性值取值范围不同的集成。

### 冗余和相关分析(Handling Redundancy in Data Integration)

一个属性的值是由其他属性值导出或计算出，那这个数据就是冗余的。一些冗余可以被相关分析检测到，对于标称数据，使用卡方检验，对于数值属性，使用*相关系数和协方差*，都可以用来评估一个属性的值，如何随另一个变化。

#### 标称数据的x°（卡方）相关检验(Correlation Analysis (Nominal Data))

p63，根据卡方检验两个属性是否独立

#### 数值相关的相关系数(Correlation Analysis (Numeric Data))

P63，计算两个属性的相关系数

#### 数值数据的协方差

评估两个属性如何一起变化，协方差为0蕴含独立性，为正：正相关，为负：负相关

简化计算：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240624101739452.png" alt="image-20240624101739452" style="zoom:40%;" />

### 元组重复

元组级检测重复

### 数据值冲突的检测与处理

对于同一实体，属性值不同（可能由于表示、尺度或编码不同而导致的）

## 数据归约

*将海量数据集进行规约表示，使其变小很多，但是仍接近于保持原始数据的完整性。

### 数据归约的策略概述（Data Reduction Strategies）

1. 维归约（Dimensionality维 reduction）

   减少所考虑的随机变量或属性的个数。方法包括小波变换和主成分分析，把原始数据变换或投影到较小的空间。

2. 数量归约（Numerosity数量 reduction）

   用替代的、较小的数据表示形式替换原数据。

3. 数据压缩（Data compression）

   使用变换，以便得到愿数据的归约或“压缩”表示。

   无损：愿数据能够从压缩后的数据重构，而不损失信息。

   有损：只能近似重构。

### 傅立叶变换（Fourier transform）

### 小波变换（Wavelet transform ）Why Wavelet Transform?

1. 去噪
2. 突出目标
3. 多分辨分析（横向、竖向）
4. 高效
5. 只能低维

### 主成分分析（PCA）Principal Component Analysis

最常用的线性成分分析方法

寻找数据的主轴方向，由主轴构成一个新的坐标系（维数可比原来低），然后数据由原坐标向新坐标系投影。

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/sjwj1.png" style="zoom:30%;" />

### 属性子集选择（Attribute Subset Selection）

Redundant attributes 冗余属性／特征；Irrelevant attributes跟任务不相关的特征／属性

删除不相关或冗余的属性来减少数据量。属性子集选择的目标是找出最小属性集，使得数据类的概率分布尽可能地接近使用所有属性得到的原分布。在缩小的属性集上挖掘还有其他的优点：它减少了出现在发现模式上的属性数目，使得模式更易于理解。

> Data Reduction 2: Numerosity Reduction
>
> 1. Parametric methods（回归）
> 2. Non-parametric methods

属性选择中的启发式搜索（Heuristic Search in Attribute Selection：显著性检验、逐步取优、逐步删除最差、组合、回溯

### 回归和对数线性模型：参数化数据规约

>对数据建模，对其参数化，模拟到函数中。
>
>包括线性回归（Linear regression）、多元回归（Multiple regression）、对数线性模型（Log-linear model）。

### 直方图（Histogram Analysis）

使用分箱来近似数据分布。

划分规则：

* 等宽：每个桶的宽度区间一致
* 等频：使得每个桶的频率粗略的为常数（即，每个桶大致包含相同个数的近邻数据样本）

### 聚类（Clustering）

### 抽样（Sampling）

允许用数据小得多的随机样本（子集）表示大型数据集。

抽样方法（Types of Sampling）

* 无放回简单随机抽样

  概率都是1/n

* 有放回简单随机抽样

* 簇抽样

* 分层抽样

  划分成几个互不相干的部分，称做“层”

### 数据立方体聚集（Data Cube Aggregation）

## 数据变换与数据离散化

### 数据变换策略（Data Transformation Strategies Overview）

1. 光滑（Smoothing）：去噪，包括分箱、回归和聚类。
2. 属性构造（Attribute/feature construction）：可以由给定的属性构造新的属性并添加到属性集中，以帮助挖掘过程。
3. 聚集（Aggregation）：对数据进行汇总或聚集
4. 规范化（Normalization）：按比例缩放，使之落到一个特定的区间。
5. 离散化（Discretization）：数值属性的原始值用区间标签或概念标签代替。
6. 由标称数据产生概念分层

### 通过规范化变换数据

最大-最小规范化：

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240415104757847.png" style="zoom:40%;" />

其中包括：（分别用标准差和均值绝对偏差）

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240415104923206.png" style="zoom:40%;" />

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240415105309157.png" style="zoom:40%;" />

### 离散化（Discretization）

* 分箱离散化
* 直方图离散化：等宽直方图、等频直方图
* 通过聚类、决策树和相关分析离散化

### 标称数据的概念分层产生

## 数据仓库与联机分析处理（Data Warehousing and On-line Analytical Processing）

### ⚠️考试（关键特征）：

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-15%2011.25.22.png" style="zoom:40%;" />

与数据库系统相比（联机事务处理，Online Transaction Processing，OLTP）侧重于数据的分析（OLAP）

### Three Data Warehouse Models：

* 企业仓库Enterprise warehouse
* 数据集市Data Mart
* 虚拟仓库Virtual warehouse

### 数据提取、变换和装入

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-22%2010.32.30.png" style="zoom:40%;" />

### 元数据库

metadata：关于数据的数据

## 数据仓库建模：数据立方体和OLAP

### 数据立方体：一种多维数据模型（Data Cube: A Multidimensional Data Model——From Tables and Spreadsheets to Data Cubes）

* 维表，Dimension tables，维：一个单位想要记录的透视或实体、属性。
* 事实表，Fact table，通常，多维数据模型围绕诸如销售这样的中心主题组织。主题用事实表表示。事实是数值度量的。

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-22%2010.53.53.png" style="zoom:50%;" />

`高纬对某个纬度进行汇总变成更低一维的方体`

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-22%2010.56.42.png" style="zoom: 33%;" />

### 星形Star schema、雪花形Snowflake schema和事实星座Fact constellations：多维数据模型的模式

星形：

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-22%2011.08.22.png" style="zoom:50%;" />

雪花形：（星形的变形，将维表进一步细分）

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240624143318072.png" alt="image-20240624143318072" style="zoom:50%;" />

### 维：概念分层的作用

### 度量的分类和计算（Measures: Their Categorization and Computation）

* 分布的（distributive):将数据分布成不同类别来计算和不分类计算是相同的，比如count(),min(),max()都是分布聚集函数。
* 代数的（algebraic):能用一个具有M个参数的代数函数计算，而每个参数都可以用一个分布函数求得，则它是代数的。比如avg()（平均值），可用sum()/count()来计算。
* 整体的（holistic)：包括中位数(median())，众数(mode())和rank()

### 典型的OLAP操作（Typical OLAP Operations）

* 上卷(roll-up):又称上钻（drill-up）通过沿一个维的概念分层向上攀升或者通过维归约在数据立方体上进行聚集。
* 下钻(drill-down):上卷逆操作.
* 切片(slice)和切块(dice): 在一个纬上操作进行切片，在两个或多个纬上操作进行切块。
* 转轴(pivot): 又称旋转(rotate),是一种目视操作，转动数据的视角，提供数据的替代表示。

### 查询多维数据库的星网查询模型

基于星网模型，从中心出发的射线组成，其中每一条射线代表一个维的概念分层。概念分层上每个“抽象级”称为一个足迹(footprint)。

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-22%2011.48.48.png" style="zoom:50%;" />

## 数据仓库的设计与使用

### 数据仓库的设计的商务分析框架

* 自顶向下视图，Top-down view
* 数据源视图，Data source view
* 数据仓库视图，Data warehouse view
* 商务查询视图，Business query view 

### 数据仓库用于信息处理

* 信息处理，Information processing
* 分析处理，Analytical processing
* 数据挖掘，Data mining

从联机分析处理到多维数据挖掘，From On-Line Analytical Processing (OLAP) to On Line Analytical Mining (OLAM)

* 数据仓库中数据的高质量，High quality of data in data warehouses
* 环绕数据仓库的信息处理基础设施，Available information processing structure surrounding data warehouses
* 基于OLAP的多维数据探索，OLAP-based exploratory(探索) data analysis
* 数据挖掘功能的联机选择，On-line selection of data mining functions

## 数据仓库的实现

### 数据立方体的有效计算、概述

### 冰山立方体：

数据立方体，只存放其聚集值（如count）大于某个最小支持度阈值的立方体单元

### ⚠️考试

## OLAP查询的有效处理

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240624154359333.png" alt="image-20240624154359333" style="zoom:50%;" />

`知道小的维度可以推出大的维度`

* 对于方体1:item_name和city都可以分别上卷到brand和province_or_city，因此利用方体1是可以查寻到的
* 对于2，因为contry维度更大，下钻并不能得到province_or_city，所以不行
* 对于3，完全匹配，可以
* 4，同理可以

## 数据泛化

### 面向属性的归纳

从概念上讲，数据立方体可以看做一种多维数据泛化。一般而言，数据泛化通过把相对低层的值（例如，属性年龄的数值）用较高层概念（例如，青年、中年和老年）替换来汇总数据，或通过减少维数，在涉及较少维数的概念空间汇总数据（例如，在汇总学生组群时，删除生日和电话号码属性）。给定存储在数据库中的大量数据，能够以简洁的形式在更一般的（而不是在较低的）抽象层描述数据是很有用的。允许数据集在多个抽象层泛化，便于用户考察数据的一般性质。

* 数据特征的面向属性的归纳：可以泛化的将属性维度提高进行泛化，不能泛化的直接删除（每个值都不一样，比如名字）
* 类比较的面向属性归纳：泛化在目标类上进行，形成主对比类关系。

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-29%2011.27.31.png" style="zoom: 50%;" />

# 4.作业：

4.2（星网查询不写）4.5（c不写）4.6，4.16

# 数据立方体技术

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-29%2011.39.34.png" style="zoom:50%;" />

基本方体的单元是基本单元，非基本方体的单元是聚集单元。聚集单元在一个或多个维上聚集。

完全数据立方体是数据仓库中的一个概念，用于描述包含所有可能的维度值组合并且每个组合都至少包含一个基本单元的数据集合

1. 祖先后代单元：低纬单元是高纬单元的祖先，（低纬：聚集程度更高）

   `如果两个单元同时有一个维度没有被聚集，且该维度取值不同，比如时间维度，一个是第一季度，另一个是第二季度，那么这个不能称为祖先后代的关系`

   绿色箭头部分就不是：

   <img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-29%2011.47.35.png" style="zoom: 33%;" />

2. 父子单元：两个祖先后代单元只差一个维度

冰山立方体：在某种情况下，只有几个高维度的（白色立方体）有值，就导致很多其他立方体是0，也就没有意义，所以需要对这个立方体聚集出都有意义的立方体，用下面的条件，这样的**部分物化**立方体称作冰山立方体。

![image-20240429115921259](https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%88%AA%E5%B1%8F2024-04-29%2011.59.14.png)

最小阈值称为最小支持度阈值（min_sup）

close cell：不存在后代单元的值是和其本身的值相同

### 数据立方体的计算的一般策略：p124

### 数据立方体计算方法

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240506104857040.png" style="zoom:40%;" />

完全或部分数据立方体的预计算可以大幅度降低响应时间，提高联机分析处理的性能

## ⚠️考试：MUltiWay完全立方体计算的多路数组聚集（适用于低维度）

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240506111054082.png" style="zoom: 40%;" />

本质就是**按照不同的聚集顺序，会有不同的块重复用到的次数不同，但是由于每个维度的值是大小不同的，甚至差异很大，因此我们尽量将用的重复次数多的块调整到内存占用最少的块中**，因此就有最佳顺序出现，重复次数少的每次计算完（实际上是最先能聚集得到的，且占用内存量最大，我们直接将其存储到硬盘，释放内存重复利用）：参照上图中的BC面，将A聚集后（读取顺序为1-2-3-4）得到BC且内存占用为100*1000最大，将其释放，再读取5-6-7-8再次聚集出BC，重复直到13才能聚集出AC（将B聚集），读到16之后，就可以得到下面的4个块，从而聚集出C，参照右图的树，同理可得。

## BUC：从顶点方体向下计算冰山立方体

<img src="https://raw.githubusercontent.com/suimanman/imgs/main/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/image-20240506114244786.png" style="zoom:50%;" />

计算稀疏立方体，自顶向下，不像第一个方法，不能利用共享数据来计算。但是可以进行Apriori剪枝。

剪枝：设置一个维数的阈值，小于这个阈值的直接剪掉。

如上图，我们分别在不同维度上进行分解查看，并同时进行剪枝。

### BUC性能分析

* BUC算法中采用了分治策略，优点在于能够分担划分开销，减少不必要的计算消耗。
* BUC的性能容易受到维的次序以及不平衡数据的影响，应当以维基数的递减顺序进行划分（优化：排序、散列和分组技术）
* BUC不像多路数组聚集（MultiWay），不能利用父子关系共享聚集计算。

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240624194917084.png" alt="image-20240624194917084" style="zoom:30%;" />

### 使用动态星树结构计算冰山立方体——star cubing

引入共享维

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240624200837880.png" alt="image-20240624200837880" style="zoom:50%;" />

### 完全立方体

* 随着元组数和维度的增加，Star-Cubing和Multi-Way表现性能较好，其中Star-Cubing略优;
* BUC性能较差，由于在划分时开销较大，导致计算密集数据时，计算时间较高。

### 冰山立方体

* BUC和Star-Cubing在稀疏数据中计算性能较好，其中Star-Cubing略优;
* 随着最小支持度减小，Star-Cubing的性能越优于BUC；
* Star-Cubing随着基数的变化保持稳定，而BUC随着基数的增加计算时间逐渐下降

# 六、挖掘频繁模式、关联和相关性：基本概念和方法

* 频繁模式Frequent pattern：频繁地出现在数据集中的模式
* frequent itemsets(频繁项集) and association rule(关联规则) mining
* 项集Itemset：一个或多个项的集合（事务集）
* 绝对支持度：一个事务出现的次数
* 相对支持度：一个事务出现的概率
* 闭频繁项集：如果 X 是频繁的并且不存在超级模式 Y כ X，且具有与 X 相同的支持，则项集 X 是封闭的，An itemset X is closed if X is frequent and there exists no super-pattern Y כ X, with the same support as X 
* 极大频繁项集：该项集是闭的并且是频繁的
* 最大模式：如果 X 是频繁的且不存在频繁超模式 Y כ X，则项集 X 是最大模式，An itemset X is a max-pattern if X is frequent and there exists no frequent super-pattern Y כ X 

### 挖掘步骤

* （1）找出所有的频繁项集：根据定义，这些项集的每一个频繁出现的次数至少与预定义的最小支持计数 min_sup一-样。
* （2）由频繁项集产生**强关联规则**：根据定义，这些规则必须满足最小支持度和最小置信度

**相对支持度大于一个`阈值`时称作频繁模式**，阈值：最小支持度

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240513104900718.png" alt="image-20240513104900718" style="zoom:40%;" />

## ⚠️考试

## 6.1Apriori（先验）:

频繁子集的非空子集必是频繁的。

算法从一维开始找，找频繁一项集，对频繁一项集进行连接找频繁二项集。

>是否可连接：比如对于一个集合{abc,abe,ade}中，abc和abe都有ab那么这两个就是可连接的。

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240513114430344.png" alt="image-20240513114430344" style="zoom: 33%;" />

根据是否可连接找出频繁项集的候选项。

再看其非空子集是否是频繁的（大于最小支持度）。

## ⚠️考试：6.2 FPGrowth（ A Frequent Pattern-Growth）

采用局部频繁模式从短的模式生长成长的模式

深度优先，不需要产生候选项

Benefits of the FP-tree Structure：Completeness 完备的、Compactness紧凑的

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240520105825143.png" alt="image-20240520105825143" style="zoom:40%;" />

把不频繁一项集的去掉，利用剩余的构建树，建立Header Table,标上数量。

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240520110832410.png" alt="image-20240520110832410" style="zoom:40%;" />

⚠️考：挖掘频繁模式：

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240520111259472.png" alt="image-20240520111259472" style="zoom:50%;" />

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240520111926548.png" alt="image-20240520111926548" style="zoom:50%;" />

### FP-G 算法的优点

* 1.不产生候选集；

* 2.使用FP-G 结构表示一个无序的项集很合理；

* FP-G 算法对不同长度的规则都有很好的适应性；

* 通过FP-树数据结构对原始数据进行压缩，效率较高；

* FP-G 算法只需进行两次事务数据库扫描，避免I/Ｏ瓶颈。

  **FP-G 算法的缺点**

* 利用FP-G 算法表示一个有序的项集并进行挖掘很困难；

* FP-G 算法对由原始数据得到的FP-树分支庞大的情况，会造成存储压力。

## 6.3 ECLAT

## 6.4 新的指标：提升度

支持度和置信度有一定误导性，

<img src="/Users/wangmeice/Library/Application Support/typora-user-images/image-20240520113821830.png" alt="image-20240520113821830" style="zoom:33%;" />

共6种模式评估度量

其他度量：

![image-20240525215630473](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240525215630473.png)

![image-20240525215847160](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240525215847160.png)

![image-20240525214826364](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240525214826364.png)

# 8. 分类

* 有监督（分类）Supervised learning (classification)和无监督（聚类）Unsupervised learning (clustering)

## 8.1 决策树

**核心**：ID3算法最早是由罗斯昆（J. Ross Quinlan）于1975年在悉尼大学提出的一种分类预测算法，算法的核心是“信息熵”。ID3算法通过计算每个属性的信息增益，认为信息增益高的是好属性，每次划分选取信息增益最高的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。

信息熵：信息的混乱程度。

在决策树中选择属性顺序时，算每一个属性的信息增益，就是看将这个属性分类后的信息熵的降低程度。跨度最大的就先选。

![image-20240527112757599](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240527112757599.png)

信息增益量越大，信息不确定性减少的程度越大，这个属性作为一棵树的根节点就能使这棵树更简洁。由此我们可以知道决策树的构造原则在于选择信息增益最大的属性特征。

但是问题在于，对于一个属性来说，其分类的类别越多，其往往分类完后信息熵会更新，也就是信息增益会越大。就会偏向具有较多值的属性。

### ID3算法优点： 

* 1.假设空间包含所有的决策树，搜索空间完整。  
* 2.健壮性好，不受噪声影响。 
* 3.可以训练缺少属性值的实例。

### ID3算法不足：  

* 1.ID3只考虑分类型的特征，没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 
*  2.ID3算法对于缺失值没有进行考虑。  
* 3.没有考虑过拟合的问题。
*  4.信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。  
* 5.划分过程会由于子集规模过小而造成统计特征不充分而停止

改进：

![image-20240527113617602](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240527113617602.png)

gini index：（和信息熵类似）

![image-20240527114846487](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240527114846487.png)

## 8.2 ⚠️考试：朴素贝叶斯

增量式：当增加新的训练样本时，不需要将训练样本合并重新学习。

![image-20240603103047780](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603103047780.png)

哪个概率最大就分为那一类。

方便比较，可直接比较分子：

![image-20240603103551643](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603103551643.png)

计算时：**假设属性之间条件独立：**

![image-20240603103719202](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603103719202.png)

例题：

![image-20240603104433180](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603104433180.png)

![image-20240603104456930](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603104456930.png)

存在问题：概率可能为0，比如“yes”中没有age"<=30”的人，那概率就是0，乘完也是0，显然不合理。

解决：拉普拉斯校准，对为0的属性进行+1（其他不加）该属性所有取值都+1，总数也+1.

![image-20240603105215724](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603105215724.png)

# 10 聚类分析

类内相似性高，类间相似性低

### ⚠️考试

![image-20240603112101887](/Users/wangmeice/Library/Application Support/typora-user-images/image-20240603112101887.png)
